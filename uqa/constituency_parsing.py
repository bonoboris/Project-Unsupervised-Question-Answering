"""Constituency parsing using benepar model in Spacy pipeline."""

from os import path
import itertools
import random as rd
from typing import Iterable, Generator, Callable, Optional, List,Dict

import numpy as np
import spacy
from benepar.spacy_plugin import BeneparComponent

from spacywrapper import SpacyFrenchModelWrapper
from data import json_loader_shuffle, DATA_PATH, count_json_files, json_loader, json_dumper
from string_utils import decorate, yellow
from list_utils import first_segment_where, rd_it, find_subseq

#--------------- Contexts ---------------# 

class Context(object):
    """Represent a context with its information.
    
    Attributes
    ----------
        fpath: The file from which this context was red from
        doc_id: The context's document (article) id
        doc_title: The context's document title
        context_id: The context's index in its document
        text: The context raw text
        ner: Named entities - list of dict with format {"start":int, "end": int, label:"str"}   
        constituency: Constiuent - list of dict with format {"start":int, "end": int, label:"str"}   
    """
    def __init__(self, fpath: str, doc: Dict, context: Dict):
        self.fpath: str = fpath
        self.doc_id: int = doc["id_doc"]
        self.doc_title: str = doc["title"]
        self.context_id: int = context["id_context"]
        self.text: str = context["text"]
        self.ner: list = context.get("entities", list())
        self.constituency = context.get("constituents", list())
        self.cloze_qa = list()
    
    def pretty_title(self):
        """Returns the documents id and title aas well as the context id and the file path as a string.""" 
        return f"{self.doc_title} [{self.doc_id}] - C{self.context_id} ({self.fpath})"
    
    def pretty_text(self):
        """Decorates and returns the text with the named entities and constituents."""
        return decorate(self.text, self.constituency + self.ner)

    def set_ner_color(self, color: str) -> str:
        """Override color for all named entites.
        
        Args
        ----
            color: one of "black", "red", "green", "yellow", "blue", "magenta", "cyan", "white"
        """
        if self.ner:
            for ent in self.ner:
                ent["color"] = color

    def set_constituency_color(self, color:str) -> str:
        """Override color for all constituents.

        Args
        ----
            color: one of "black", "red", "green", "yellow", "blue", "magenta", "cyan", "white"
        """
        if self.constituency:
            for el in self.constituency:
                el["color"] = color

    def get_cloze_qa_str(self):
        s = ""
        for cq, ca in self.cloze_qa:
            cq_text = self.text[cq["start"]: cq["stop"]]
            ca["offset"] = cq["start"]
            s += decorate(cq_text, [ca]) + "\n"
        return s

    def __str__(self):
        s = yellow(self.pretty_title()) + "\n"
        s += self.pretty_text()
        return s

def rd_context_gen(json_file_it: Iterable[Dict], filter_empty_ner: bool =False) -> Generator[Context, None, None]:
    """Shuffle article in each file and contexts in each article and yield the contexts as Context objects.
    
    Notes
    -----
        The randomization is per file, per article, every context of the first file in `json_file_it` are yielded before the ones from the second file,
        and every context of an article are yieled before going to the next one.
    """
    for fpath, fcontent in json_file_it:
        for article in rd_it(fcontent):
            for context in rd_it(article["contexts"]):
                if filter_empty_ner and not context["entities"]:
                    continue
                yield Context(fpath, article, context)

#--------------- Constituency parsing ---------------# 

def constituency_gen_context(
        context_it: Iterable[Context],
        filter_callback: Callable[[spacy.tokens.Doc, Context], List[Dict]],
        cloze_question_callback: Optional[Callable[[Context], List[Dict]]] = None
    ) -> Generator[Context, Optional[str], None]:
    """Parse the context constituents, filters them, add them to the context and yield the context.  
    
    Args
    ----
        context_it: Context iterable
        filter_callback:
            A function taking spacy Doc object generated by the pipeline and the context as arguments and returning the
            list of constituents to store in the context.
        cloze_qestion_callback:
            A function taking a Context object and returning a list of cloze questions.
    Yields
    ------
        context with `constituency` attribut set.
    SendArgs
    --------
        skip: None, "article", "file"
            Skip the current article or the current file
    """
    SpacyFrenchModelWrapper.model.add_pipe(BeneparComponent("benepar_fr"))

    def _parse(context):
        return SpacyFrenchModelWrapper.model(context.text, disable=["ner", "tagger"]) 

    skip = None
    skip_file = None
    skip_art = None
    skip_cnt = 0
    for context in context_it:
        context = next(context_it)
        if skip_file is not None:
            if context.fpath == skip_file:
                skip_cnt += 1
                continue
            else:
                print(f"{skip_cnt} contexts skipped.\n")
                skip_file = None
        if skip_art is not None:
            if context.doc_id == skip_art:
                skip_cnt += 1
                continue
            else:
                print(f"{skip_cnt} contexts skipped.\n")
                skip_art = None
        
        doc = _parse(context)
        context.constituency = filter_callback(doc, context)
        if cloze_question_callback:
            context.cloze_question = cloze_question_callback(context) 
        skip = yield context
        if skip is not None:
            if skip == "article":
                skip_art = context.doc_id
            elif skip == "file":
                skip_file = context.fpath
            skip = None
            skip_cnt = 0

#--------------- Utils ---------------# 

def span_to_dict(span, depth=0, no_color=False):
    """Convert a spacy Span object into a dictionnary with entries `start`, `end` and `label`.
    
    If `no_color` is False, a `color` entry which value depends on `depth` is also added in the dictionnary.
    """
    colors = ["red", "magenta", "blue", "cyan", "yellow"]
    d = dict(start=span.start_char, end=span.end_char, label= "|".join(span._.labels))
    if not no_color:
        d["color"] = colors[depth % len(colors)]
    return d


def in_span_factory(span: spacy.tokens.Span) -> Callable[[Dict], bool]:
    """Returns a predicate which test whether a dictonnary representing a span is inside the `span` argument.
    
    Args
    ----
        span: A Spacy span object
    Returns
    -------
        f: Predicate on dictionnaries with enties `start` and `end`, returns true if [`start`, `end`) is included in `span`.
    """
    return lambda ent: ent["start"] >= span.start_char and ent["end"] <= span.end_char


def input_continue(msg = "Continue ?", default_yes=True):
    if default_yes:
        c = input(msg + " [Y/n] ")
        if c.lower().strip() == "n":
            exit()
        else:
            return c
    else:
        c = input(msg + "[y/N] ")
        if c.lower().strip() != "y":
            exit()
        else:
            return c

def input_continue_skip():
    c = input("(N)ext / (q)uit / next (a)rticle / next (f)ile ? ")
    c = c.lower().strip()
    if c == "q":
        exit()
    ret = None
    if c == "a":
        ret = "article"
    elif c == "f":
        ret = "file"
    return ret

class Fake(object):
    pass

#--------------- Filtering functions ---------------# 

def d1_const(doc: spacy.tokens.Doc, context: Context=None) -> List[Dict]:
    """Returns only and all the first level constituents. (`context` arg has no effect)"""
    const = list()
    for sent in doc.sents:
        for span in sent._.children:
            const.append(span_to_dict(span))
    return const

def d1_and_ascending_ent(doc: spacy.tokens.Doc, context: Context=None) -> List[Dict]:
    """Returns all first level constituents and all constituent containing a named entity."""
    ents = context.ner
    
    const = list()
    ents = sorted(ents, key=lambda ent: ent["start"])

    def _rec(span, start=0, stop=len(ents), depth=0):
        for child in span._.children:
            i, j = first_segment_where(ents, in_span_factory(child), start, stop)
            if i >= 0:
                const.append(span_to_dict(child, depth))
                start = i
                _rec(child, i, j, depth= depth + 1)

    cur_ent = 0
    for sent in doc.sents:
        for span in sent._.children:
            const.append(span_to_dict(span))
            i, j = first_segment_where(ents, in_span_factory(span), cur_ent)
            if i >= 0:
                _rec(span, i, j, depth=1)
                cur_ent = j
    return const


def filter_np_vn_npats(doc: spacy.tokens.Doc, context: Context=None) -> List[Dict]:
    """Returns all first level constituents and all constituent containing a named entity
        for sentences containing a sub-sequence of first level constituents 'NP-SUJ', 'VN', 'NP-ATS' and either 'NP-SUJ' or 'NP-ATS' contains a named entity.
    """
    # This function body filter at sentence level and use d1_and_ascending_ent to filters constituents at sub-sentence level
    
    ents = context.ner
    def _filter_sent(sent):
        children = list(sent._.children)
        d1_labels = ["|".join(span._.labels) for span in children]
        for target in [['NP-SUJ', 'VN', 'NP-ATS']]:
            npsuj = find_subseq(d1_labels, target)
            if npsuj > -1:
                npats = npsuj + 2
                in_npsuj = in_span_factory(children[npsuj])
                in_npats = in_span_factory(children[npats])
                for ent in ents:
                    if in_npsuj(ent) or in_npats(ent) :
                        return True
        return False
    
    f = Fake()  # Fake spacy.tokens.Doc
    f.sents = list()
    for sent in doc.sents:         
        if _filter_sent(sent):
            f.sents.append(sent)
    return d1_and_ascending_ent(f, context)

def filter_npsuj_vn_npats_1_ent_in_npsuj(doc: spacy.tokens.Doc, context: Context=None) -> List[Dict]:
    """Returns only sub-sequences NP-SUJ, VN, NP-ATS where NP-SUJ contains a single named entity."""
    ents = context.ner

    def _filter_sent(sent):
        children = list(sent._.children)
        d1_labels = ["|".join(span._.labels) for span in children]
        for target in [['NP-SUJ', 'VN', 'NP-ATS']]:
            npsuj = find_subseq(d1_labels, target)
            if npsuj > -1:
                npats = npsuj + 2
                in_npsuj = in_span_factory(children[npsuj])
                ents_in_npsuj = [in_npsuj(ent) for ent in ents]
                if ents_in_npsuj.count(True) == 1:
                    return [span_to_dict(span) for span in children[npsuj: npsuj + 3]], ents_in_npsuj.index[True]

        return list(), None
    
    const = list()
    for sent in doc.sents:
        const_, ent_ = _filter_sent(sent)
        if const:
            const.extend(const_)
            
            cloze_q = dict(start = const_[0]["start"], end = const_[-1]["end"], label="CQ", color="red")
            cloze_a = dict(**ent_)
            cloze_a["color"] = "green"
            cloze_a["label"] = f"CA|{cloze_a['label']}"
            context.cloze_qa.append((cloze_q, cloze_a))
    return const

# #--------------- Cloze question generation function ---------------#
# # The following functions must take a Context object as their first argument and returns a list of dictionnaries
# # Each dictionnary should characterize either a cloze question or its answer and must have the entries:
# #   - start: int
# #   - end: int
# #   - label: str
# #   (- color: str | Optional) 

# def from_npsuj_vn_npats(context: Context):
#     if not context.constituency:
#         return list()
#     else: 

def test_d1_and_ascending_ent():
    def span(start, end, label):
        obj = Fake()
        obj.start_char = start
        obj.end_char = end
        obj._ = Fake()
        obj._.labels = [label]
        obj._.children = []
        return obj
    
    s0 = span(0, 4, "s0")
    s1 = span(4, 15, "s1")
    s2 = span(4, 10, "s2")
    s3 = span(5, 8, "s3")
    s4 = span(10, 15, "s4")
    s5 = span(15, 26, "s5")
    s6 = span(15, 25, "s6")
    s7 = span(15, 20, "s7")
    s8 = span(20, 25, "s8")

    s2._.children.append(s3)
    s1._.children.append(s2)
    s1._.children.append(s4)
    s6._.children.append(s7)
    s5._.children.append(s6)
    s5._.children.append(s8)

    sent = Fake()
    sent._ = Fake()
    sent._.children = [s0, s1, s5]

    doc = Fake()
    doc.sents = [sent]

    ents = [
        {
            "start": 5,
            "end": 9,
            "label": "e1"
        },
        {
            "start": 18,
            "end": 20,
            "label": "e2"
        },
        {
            "start": 22,
            "end": 24,
            "label": "e3"
        }
    ]
    const = d1_and_ascending_ent(doc, ents)
    print(const)


def one_by_one(filtering_function=filter_np_vn_npats):
    pass


def all_cloze_questions(
        filtering_function=filter_npsuj_vn_npats_1_ent_in_npsuj,
    ):
    rd_file_it = json_loader_shuffle(path.join(DATA_PATH, "ner_json"))
    rd_context_it = rd_context_gen(rd_file_it, filter_empty_ner=True)
    context_gen = constituency_gen_context(rd_context_it, filter_np_vn_npats)
    for context in context_gen:
        if context.cloze_qa:
            print(context.get_cloze_qa_str())


def main(filtering_function=filter_np_vn_npats):
    """Iterate over the contexts, parse the constituents
    and display the context for which at least one sentence constituents are not filtered by filtering_function."""
    rd_file_it = json_loader_shuffle(path.join(DATA_PATH, "ner_json"))
    rd_context_it = rd_context_gen(rd_file_it, filter_empty_ner=True)
    context_gen = constituency_gen_context(rd_context_it, filtering_function)
    skip = None
    try:
        while True:
            cont = context_gen.send(skip)
            skip = None
            if cont.constituency:
                cont.set_ner_color("green")
                # cont.set_constituency_color("red")
                print(cont)
                print()
                skip = input_continue_skip()
                print()
    except StopIteration:
        pass


if __name__ == "__main__":
    main(d1_and_ascending_ent)


# ---------------------------------------------------

class ConstituentsTree(object):
    def __init__(self, doc=None):
        self.roots = list() 
        if doc:
            self.roots = constituents_tree_dict(doc)

    def to_json(self):
        return self.roots
    
    @classmethod
    def from_json(cls, roots):
        inst = cls()
        inst.roots = roots
        return inst
    
    def to_fake_spacy_doc(self):
        def _rec(dct):
            g = Fake()
            g.start_char = dct["start"]
            g.end_char = dct["end"]
            g_ = Fake()
            g_.labels = dct["label"].split()
            g_.children = [_rec(child) for child in dct["children"]]
            g._ = g_
            return g
        
        f = Fake()
        f.sents = list()
        for root in self.roots:
            f.sents.append(_rec(root))
        return f

def constituents_tree_dict(doc: spacy.tokens.Doc):
    def make_tree(span: spacy.tokens.Span, label:Optional[str]=None):
        dct = dict()
        dct["start"] = span.start_char
        dct["end"] = span.end_char
        dct["label"] = label or "|".join(span._.labels)
        dct["children"] = [make_tree(child) for child in span._.children]
        return dct

    consts = list()
    for sent in doc.sents:
        consts.append(make_tree(sent))
    return consts


def constituency_gen(json_file_it):
    json_file_it, json_file_it_copy = itertools.tee(json_file_it)
    context_it = (context["text"] for _, json_file in json_file_it_copy for article in json_file for context in article["contexts"])
    SpacyFrenchModelWrapper.model.add_pipe(BeneparComponent("benepar_fr"))
    docs_it = SpacyFrenchModelWrapper.model.pipe(context_it, disable=["ner", "tagger"])
    for file_path, json_file in json_file_it:
        print(f"*** File {file_path} ***")
        for json_article in json_file:
            print(f"*** Article {json_article['id_doc']} {json_article['title']}")
            for context in json_article["contexts"]:
                doc = next(docs_it)
                context["constituency"] = ConstituentsTree(doc).to_json()
        yield file_path, json_file


def foo():
    dirpath = path.join(DATA_PATH, "ner_json")
    loader = json_loader(dirpath)
    cg = constituency_gen(loader)
    cont = next(cg)
    const = cont["constituency"]
    fdoc = ConstituentsTree.from_json(cont["constituency"]).to_fake_spacy_doc()
    rconst = ConstituentsTree(fdoc).to_json()
    print(const == rconst)


def parse_and_save_constituents():
    dirpath = path.join(DATA_PATH, "ner_json")
    file_count = count_json_files(dirpath)
    print("File count", file_count)
    loader = json_loader(dirpath)
    const_it = constituency_gen(loader)
    out_it = ((fpath.replace("ner_json", "const_json"), fjson) for fpath, fjson in const_it)
    print(next(out_it))
    exit()
    json_dumper(out_it)

if __name__ == "__main__":
    foo()
