"""Constituency parsing using benepar model in Spacy pipeline."""

from os import path
import itertools
import random as rd
from typing import Iterable, Generator, Callable, Optional, List,Dict

import spacy
from benepar.spacy_plugin import BeneparComponent

from spacywrapper import SpacyFrenchModelWrapper
from data import json_loader_shuffle, DATA_PATH
from string_utils import decorate, yellow
from list_utils import first_segment_where, rd_it, find_subseq

#--------------- Contexts ---------------# 

class Context(object):
    """Represent a context with its information.
    
    Attributes
    ----------
                print(".", end="", flush=True)
        fpath: The file from which this context was red from
        doc_id: The context's document (article) id
        doc_title: The context's document title
        context_id: The context's index in its document
        text: The context raw text
        ner: Named entities - list of dict with format {"start":int, "end": int, label:"str"}   
        constituency: Constiuent - list of dict with format {"start":int, "end": int, label:"str"}   
    """
    def __init__(self, fpath: str, doc: Dict, context: Dict):
        self.fpath: str = fpath
        self.doc_id: int = doc["id_doc"]
        self.doc_title: str = doc["title"]
        self.context_id: int = context["id_context"]
        self.text: str = context["text"]
        self.ner: list = context.get("entities", list())
        self.constituency = context.get("constituents", list())
    
    def pretty_title(self):
        """Returns the documents id and title aas well as the context id and the file path as a string.""" 
        return f"{self.doc_title} [{self.doc_id}] - C{self.context_id} ({self.fpath})"
    
    def pretty_text(self):
        """Decorates and returns the text with the named entities and constituents."""
        return decorate(self.text, self.constituency + self.ner)

    def set_ner_color(self, color: str) -> str:
        """Override color for all named entites.
        
        Args
        ----
            color: one of "black", "red", "green", "yellow", "blue", "magenta", "cyan", "white"
        """
        if self.ner:
            for ent in self.ner:
                ent["color"] = color

    def set_constituency_color(self, color:str) -> str:
        """Override color for all constituents.

        Args
        ----
            color: one of "black", "red", "green", "yellow", "blue", "magenta", "cyan", "white"
        """
        if self.constituency:
            for el in self.constituency:
                el["color"] = color

    def __str__(self):
        s = yellow(self.pretty_title()) + "\n"
        s += self.pretty_text()
        return s

def rd_context_gen(json_file_it: Iterable[Dict], filter_empty_ner: bool =False) -> Generator[Context, None, None]:
    """Shuffle article in each file and contexts in each article and yield the contexts as Context objects.
    
    Notes
    -----
        The randomization is per file, per article, every context of the first file in `json_file_it` are yielded before the ones from the second file,
        and every context of an article are yieled before going to the next one.
    """
    for fpath, fcontent in json_file_it:
        for article in rd_it(fcontent):
            for context in rd_it(article["contexts"]):
                if filter_empty_ner and not context["entities"]:
                    continue
                yield Context(fpath, article, context)

#--------------- Constituency parsing ---------------# 

def constituency_gen_context(
        context_it: Iterable[Context],
        filter_callback: Callable[[spacy.tokens.Doc, Context], Optional[List[Dict]]]
    ) -> Generator[Context, Optional[str], None]:
    """Parse the context constituents, filters them, add them to the context and yield the context.  
    
    Args
    ----
        context_it: Context iterable
        filter_callback:
            A function taking spacy Doc object generated by the pipeline and the context as arguments and returning either None or the
            list of constituents to store in the context.
    Yields
    ------
        context with `constituency` attribut set.
    SendArgs
    --------
        skip: None, "article", "file"
            Skip the current article or the current file
    """
    SpacyFrenchModelWrapper.model.add_pipe(BeneparComponent("benepar_fr"))

    def _parse(context):
        return SpacyFrenchModelWrapper.model(context.text, disable=["ner", "tagger"]) 

    skip = None
    skip_file = None
    skip_art = None
    skip_cnt = 0
    for context in context_it:
        context = next(context_it)
        if skip_file is not None:
            if context.fpath == skip_file:
                skip_cnt += 1
                continue
            else:
                print(f"{skip_cnt} contexts skipped.\n")
                skip_file = None
        if skip_art is not None:
            if context.doc_id == skip_art:
                skip_cnt += 1
                continue
            else:
                print(f"{skip_cnt} contexts skipped.\n")
                skip_art = None
        
        doc = _parse(context)
        const = filter_callback(doc, context)
        context.constituency = const
        skip = yield context
        if skip is not None:
            if skip == "article":
                skip_art = context.doc_id
            elif skip == "file":
                skip_file = context.fpath
            skip = None
            skip_cnt = 0

#--------------- Utils ---------------# 

def span_to_dict(span, depth=0, no_color=False):
    """Convert a spacy Span object into a dictionnary with entries `start`, `end` and `label`.
    
    If `no_color` is False, a `color` entry which value depends on `depth` is also added in the dictionnary.
    """
    colors = ["red", "magenta", "blue", "cyan", "yellow"]
    d = dict(start=span.start_char, end=span.end_char, label= "|".join(span._.labels))
    if not no_color:
        d["color"] = colors[depth % len(colors)]
    return d


def in_span_factory(span: spacy.tokens.Span) -> Callable[[Dict], bool]:
    """Returns a predicate which test whether a dictonnary representing a span is inside the `span` argument.
    
    Args
    ----
        span: A Spacy span object
    Returns
    -------
        f: Predicate on dictionnaries with enties `start` and `end`, returns true if [`start`, `end`) is included in `span`.
    """
    return lambda ent: ent["start"] >= span.start_char and ent["end"] <= span.end_char


def input_continue(msg = "Continue ?", default_yes=True):
    if default_yes:
        c = input(msg + " [Y/n] ")
        if c.lower().strip() == "n":
            exit()
        else:
            return c
    else:
        c = input(msg + "[y/N] ")
        if c.lower().strip() != "y":
            exit()
        else:
            return c

def input_continue_skip():
    c = input("(N)ext / (q)uit / next (a)rticle / next (f)ile ? ")
    c = c.lower().strip()
    if c == "q":
        exit()
    ret = None
    if c == "a":
        ret = "article"
    elif c == "f":
        ret = "file"
    return ret

class Fake(object):
    pass

#--------------- Filtering functions ---------------# 


def d1_const(doc: spacy.tokens.Doc, context: Context=None) -> List[Dict]:
    """Returns only and all the first level constituents. (`context` arg has no effect)"""
    const = list()
    for sent in doc.sents:
        for span in sent._.children:
            const.append(span_to_dict(span))
    return const

def d1_and_ascending_ent(doc: spacy.tokens.Doc, context: Context=None) -> List[Dict]:
    """Returns all first level constituents and all constituent containing a named entity."""
    ents = context.ner
    
    const = list()
    ents = sorted(ents, key=lambda ent: ent["start"])

    def _rec(span, start=0, stop=len(ents), depth=0):
        for child in span._.children:
            i, j = first_segment_where(ents, in_span_factory(child), start, stop)
            if i >= 0:
                const.append(span_to_dict(child, depth))
                start = i
                _rec(child, i, j, depth= depth + 1)

    cur_ent = 0
    for sent in doc.sents:
        for span in sent._.children:
            const.append(span_to_dict(span))
            i, j = first_segment_where(ents, in_span_factory(span), cur_ent)
            if i >= 0:
                _rec(span, i, j, depth=1)
                cur_ent = j
    return const


def filter_np_vn_npats(doc: spacy.tokens.Doc, context: Context=None) -> List[Dict]:
    """Returns all first level constituents and all constituent containing a named entity
        for sentences containing a sub-sequence of first level constituents 'NP-SUJ', 'VN', 'NP-ATS' and either 'NP-SUJ' or 'NP-ATS' contains a named entity.
    """
    # This function body filter at sentence level and use d1_and_ascending_ent to filters constituents at sub-sentence level
    
    def _filter_sent(sent):
        children = list(sent._.children)
        d1_labels = ["|".join(span._.labels) for span in children]
        for target in [['NP-SUJ', 'VN', 'NP-ATS']]:
            npsuj = find_subseq(d1_labels, target)
            if npsuj > -1:
                npats = npsuj + 2
                in_npsuj = in_span_factory(children[npsuj])
                in_npats = in_span_factory(children[npats])
                for ent in ents:
                    if in_npsuj(ent) or in_npats(ent) :
                        return True
        return False
    
    f = Fake()  # Fake spacy.tokens.Doc
    f.sents = list()
    ents = context.ner
    for sent in doc.sents:                
        if _filter_sent(sent):
            f.sents.append(sent)
    return d1_and_ascending_ent(f, context)


def test_d1_and_ascending_ent():
    def span(start, end, label):
        obj = Fake()
        obj.start_char = start
        obj.end_char = end
        obj._ = Fake()
        obj._.labels = [label]
        obj._.children = []
        return obj
    
    s0 = span(0, 4, "s0")
    s1 = span(4, 15, "s1")
    s2 = span(4, 10, "s2")
    s3 = span(5, 8, "s3")
    s4 = span(10, 15, "s4")
    s5 = span(15, 26, "s5")
    s6 = span(15, 25, "s6")
    s7 = span(15, 20, "s7")
    s8 = span(20, 25, "s8")

    s2._.children.append(s3)
    s1._.children.append(s2)
    s1._.children.append(s4)
    s6._.children.append(s7)
    s5._.children.append(s6)
    s5._.children.append(s8)

    sent = Fake()
    sent._ = Fake()
    sent._.children = [s0, s1, s5]

    doc = Fake()
    doc.sents = [sent]

    ents = [
        {
            "start": 5,
            "end": 9,
            "label": "e1"
        },
        {
            "start": 18,
            "end": 20,
            "label": "e2"
        },
        {
            "start": 22,
            "end": 24,
            "label": "e3"
        }
    ]
    const = d1_and_ascending_ent(doc, ents)
    print(const)


def main(filtering_function=filter_np_vn_npats):
    """Iterate over the contexts, parse the constituents
    and display the context for which at least one sentence constituents are not filtered by filtering_function."""
    rd_file_it = json_loader_shuffle(path.join(DATA_PATH, "ner_json"))
    rd_context_it = rd_context_gen(rd_file_it, filter_empty_ner=True)
    context_gen = constituency_gen_context(rd_context_it, filter_np_vn_npats)
    skip = None
    try:
        while True:
            cont = context_gen.send(skip)
            skip = None
            if cont.constituency:
                cont.set_ner_color("green")
                # cont.set_constituency_color("red")
                print(cont)
                print()
                skip = input_continue_skip()
                print()
    except StopIteration:
        pass


if __name__ == "__main__":
    main()


# ---------------------------------------------------

# def constituency_gen(json_file_it):
#     json_file_it, json_file_it_copy = itertools.tee(json_file_it)
#     context_it = (context["text"] for _, json_file in json_file_it_copy for article in json_file for context in article["contexts"])
#     SpacyFrenchModelWrapper.model.add_pipe(BeneparComponent("benepar_fr"))
#     docs_it = SpacyFrenchModelWrapper.model.pipe(context_it, disable=["ner", "tagger"])
#     for file_path, json_file in json_file_it:
#         print(f"*** File {file_path} ***")
#         for json_article in json_file:
#             print(f"*** Article {json_article['id_doc']} {json_article['title']}")
#             for context in json_article["contexts"]:
#                 doc = next(docs_it)
#                 print(f"***Context {context['id_context']}***")
#                 print(context["text"])
#                 print()
#                 print_const_sents(doc)
#                 c = input("\nPrint next context ? [Y/n] ")
#                 if c.lower() == "n":
#                     exit()
#         yield file_path, json_file
